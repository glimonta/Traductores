#!/usr/bin/env ruby

#Gabriela Limonta 10-10385
#John Delgado 10-10196

# Un objeto de texto es una generalización de un fragmento textual en una posición
# determinada de un texto. Instancias de esta idea son los errores lexicográficos y los tokens.
class ObjetoDeTexto
  attr_accessor :linea, :columna, :texto
end

# Un error lexicográfico es un objeto que guarda la posición de un error en un contexto
# del programa
class ErrorLexicografico < ObjetoDeTexto
  # Se encarga de inicializar un error lexicografico indicandole
  # en que posicion está y por que texto está conformado
  def initialize(linea, columna, texto)
    @linea   = linea
    @columna = columna
    @texto   = texto
  end

  # Se encarga de pasar el error lexicográfico a string para imprimir
  # en pantalla.
  def to_s
    "Error: caracter inesperado \"#{@texto}\" en línea #{@linea}, columna #{@columna}."
  end
end

# La clase token es un objeto de texto que ademas de tener un contexto
# tiene una expresión regular que permite identificar al token dentro
# del contexto mayor
class Token < ObjetoDeTexto
  # Queremos que el campo regex sera una variable de la clase y no de cada subclase
  # en particular de Token por lo que declaramos regex en la clase singleton de Token
  class << self #Clase singleton
    attr_accessor :regex
  end

  attr_accessor :linea, :columna

  # Se encarga de pasar el token a string para que imprima por pantalla.
  def to_s
    "#{self.class.name} #{if [TkString].include? self.class then @texto + ' ' else '' end} #{if [TkId, TkNum].include? self.class then @texto.inspect + ' ' else '' end}(Línea #{@linea}, Columna #{@columna})"
  end
end

# Se define un diccionario que contiene las expresiones regulares para cada
# token existente.
tokens = {
  'AbreParentesis'    => /\A\(/                      ,
  'CierraParentesis'  => /\A\)/                      ,
  'Coma'              => /\A,/                       ,
  'Desigual'          => /\A\/=/                     ,
  'Division'          => /\A\/(?!=)/                 ,
  'DosPuntos'         => /\A\.\./                    ,
  'Flecha'            => /\A->/                      ,
  'Id'                => /\A([a-zA-Z_][a-z0-9A-Z_]*)/,
  'Igual'             => /\A==/                      ,
  'MayorIgualQue'     => /\A>=/                      ,
  'MayorQue'          => /\A>(?![=>])/               ,
  'MenorIgualQue'     => /\A<=/                      ,
  'MenorQue'          => /\A<(?!=)/                  ,
  'Modulo'            => /\A%/                       ,
  'Multiplicacion'    => /\A\*/                      ,
  'Num'               => /\A[0-9]+/                  ,
  'Pertenece'         => /\A>>/                      ,
  'PuntoYComa'        => /\A;/                       ,
  'Resta'             => /\A-(?!>)/                  ,
  'String'            => /\A"([^"\\]|\\[n\\"])*"/    ,
  'Suma'              => /\A\+/                      ,
  'Asignacion'        => /\A=/                       ,
}

# Se definen las palabras reservadas.
reserved_words = %w(and as begin bool bottom case declare do else end for if in int length not of or program range read rtoi then top while write writeln)

# Guardamos aqui dentro del diccionario de tokens las expresiones para las palabras reservadas.
reserved_words.each do |w|
  tokens[w.capitalize] = /\A#{w}\b/
end

# Para cada token vamos creando las nuevas subclases para cada token.
tokens.each do |name, regex|
  clase = Class::new(Token) do
   #Asignamos la expresion regular
   @regex = regex

   # Se encarga de inicializar el token en el contexto.
   def initialize(linea, columna, texto)
     @linea   = linea
     @columna = columna
     @texto   = texto
   end
  end

  # Le damos nombre a la nueva sub clase creada
  Object::const_set "Tk#{name}", clase
end

# Creamos un arreglo de tokens cuyos elementos del arreglo son las
# subclases para cada token.
$tokens = []
ObjectSpace.each_object(Class) do |o|
  $tokens << o if o.ancestors.include? Token and o != TkId and o != Token
end

# Clase Lexer que guarda la lista de tokens reconodicos, errores encontrados
# de cierta entrada y la posicion actual (fila, columna)
class Lexer
  attr_reader :tokens, :errores

  # Se encarga de inicializar el lexer en el string de entrada dado.
  def initialize(entrada)
    @tokens  = []
    @errores = []
    @entrada = entrada
    @linea   = 1
    @columna = 1
  end

  # Se encarga de consumir el string de entrada en cierta longitud.
  def consume(longitud)
    # Si la longitud es cero retornamos nil
    return if longitud.zero?

    # Guardamos los n primeros caracteres.
    consumido = @entrada[0, longitud]
    # Guardamos el numero de saltos de lineas encontrados
    lineas    = (consumido + ' ').lines.to_a.length.pred
    # y se lo sumamos a la cantidad de lineas del lexer
    @linea    += lineas
    # la entrada ahora será la cola desde la longitud dada hasta
    # la longitud original de la entrada
    @entrada  = @entrada[longitud, @entrada.length]

    # Si la cantidad de lineas es cero entonces aumentamos
    # el valor de columna tanto como longitud sea dada
    if lineas.zero? then
      @columna  += longitud
    else
      # sino agregamos la cantidad pertinente a columna.
      consumido =~ /\n(.*)\z/
      @columna  = 1 + $1.length
    end
  end

  # Se encarga de reconocer que token hace match con la entrada del lexer.
  def yylex
    # Buscamos los espacios en blanco y los consumimos
    @entrada =~ /\A(\s|\n|\/\/.*)*/
    consume($&.length)

    # Si la entrada esta vacia retornamos nil
    return nil if @entrada.empty?

    # Creamos por defecto una clase Error Lexicográfico
    new_token_class = ErrorLexicografico
    new_token_text  = @entrada[0].chr

    # Para cada tipo de token vamos a revisar a ver si sus expresiones
    # regulares hacen match
    $tokens.each do |token|
      if @entrada =~ token.regex then
        # Si tenemos un hit entonces cambiamos la clase por defecto que
        # habiamos creado como error y le ponemos como texto lo leido en entrada.
        new_token_class = token
        new_token_text  = $&
        break
      end
    end

    # Si no hicimos match con ningun token anteriormente procedemos a revisar el
    # caso especial del tokenID en caso de tener un hit cambiamos la clase a TkId
    if new_token_class == ErrorLexicografico and @entrada =~ TkId.regex then
      new_token_class = TkId
      new_token_text  = $&
    end

    # Creamos el nuevo token
    new_token = new_token_class.new(@linea, @columna, new_token_text)

    # Consumimos la longitud del texto que hizo match
    consume(new_token_text.length)

    # Si el nuevo token es un error lexicografico lo agregamos a la lista de
    # errores y si no lo es lo agregamos a la lista de tokens reconocidos.
    if new_token.is_a? ErrorLexicografico then
      @errores << new_token
    else
      @tokens << new_token
    end
  end

  # Se encarga de crear el string de salida con los to_s de cada token para imprimir el estado
  # del lexer. Si hay errores imprime estos solamente, si no hay errores imprime los tokens reconocidos.
  def to_s
    (if @errores.empty? then @tokens else @errores end).map { |token| token.to_s }.join "\n"
  end
end

# En el main se llama a los metodos declarados anteriormente
def main
  # Leemos la entrada del archivo
  entrada = File::read(ARGV[0])

  # Creamos un nuevo lexer con esta entrada
  lexer = Lexer::new entrada
  # Mientras el lexer siga leyendo e identificando tokens o errores en el texto de entrada el programa corre
  t = false
  while (!t) do
    t = lexer.yylex.nil?
  end

  # Finalmente se imprime el lexer.
  puts lexer
end

main
